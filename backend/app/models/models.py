from datetime import datetime
import uuid
from app.extensions import db

class UploadSession(db.Model):
    __tablename__ = 'upload_sessions'
    
    id = db.Column(db.String(36), primary_key=True, default=lambda: str(uuid.uuid4()))
    file_name = db.Column(db.String(255), nullable=False)
    total_size = db.Column(db.Integer, nullable=False)
    total_chunks = db.Column(db.Integer, nullable=False)
    chunks_received = db.Column(db.Integer, default=0)
    status = db.Column(db.Enum('uploading', 'combining', 'completed', 'failed'), nullable=False)
    created_at = db.Column(db.DateTime, default=datetime.utcnow)
    updated_at = db.Column(db.DateTime, default=datetime.utcnow, onupdate=datetime.utcnow)
    recordings = db.relationship('Recording', backref='upload_session', lazy=True, cascade='all, delete-orphan')

class Recording(db.Model):
    __tablename__ = 'recordings'
    
    id = db.Column(db.Integer, primary_key=True)
    upload_session_id = db.Column(db.String(36), db.ForeignKey('upload_sessions.id'), nullable=False)
    file_path = db.Column(db.String(255), nullable=False)
    file_name = db.Column(db.String(255), nullable=False)
    file_size = db.Column(db.Integer, nullable=False)
    duration = db.Column(db.Integer)
    status = db.Column(db.Enum('processing', 'completed', 'failed'), nullable=False)
    created_at = db.Column(db.DateTime, default=datetime.utcnow)
    updated_at = db.Column(db.DateTime, default=datetime.utcnow, onupdate=datetime.utcnow)
    transcription = db.relationship('Transcription', backref='recording', uselist=False, cascade='all, delete-orphan')

class Transcription(db.Model):
    __tablename__ = 'transcriptions'
    
    id = db.Column(db.Integer, primary_key=True)
    recording_id = db.Column(db.Integer, db.ForeignKey('recordings.id'), nullable=False)
    text = db.Column(db.Text, nullable=False)
    words = db.Column(db.JSON, nullable=True)  # Store word-level data
    diarized_text = db.Column(db.JSON, nullable=True)  # Store speaker-separated text
    meta = db.Column(db.JSON, nullable=True)  # Store metadata like duration, confidence
    status = db.Column(db.Enum('processing', 'completed', 'failed'), nullable=False)
    created_at = db.Column(db.DateTime, default=datetime.utcnow)
    updated_at = db.Column(db.DateTime, default=datetime.utcnow, onupdate=datetime.utcnow)

class SOAPNote(db.Model):
    __tablename__ = 'soap_notes'
    
    id = db.Column(db.Integer, primary_key=True)
    recording_id = db.Column(db.Integer, db.ForeignKey('recordings.id'), nullable=False)
    transcription_id = db.Column(db.Integer, db.ForeignKey('transcriptions.id'), nullable=False)
    subjective = db.Column(db.Text)
    objective = db.Column(db.Text)
    assessment = db.Column(db.Text)
    plan = db.Column(db.Text)
    status = db.Column(db.Enum('draft', 'finalized', 'archived'), nullable=False)
    created_at = db.Column(db.DateTime, default=datetime.utcnow)
    updated_at = db.Column(db.DateTime, default=datetime.utcnow, onupdate=datetime.utcnow) 

class ChunkSummary(db.Model):
    __tablename__ = 'chunk_summaries'
    
    id = db.Column(db.Integer, primary_key=True)
    soap_note_id = db.Column(db.Integer, db.ForeignKey('soap_notes.id'), nullable=False)
    chunk_index = db.Column(db.Integer, nullable=False)  # The order of the chunk in the conversation
    chunk_text = db.Column(db.JSON, nullable=False)  # The original chunk text (diarized utterances)
    summary = db.Column(db.Text, nullable=False)  # The summary generated by GPT
    created_at = db.Column(db.DateTime, default=datetime.utcnow)
    updated_at = db.Column(db.DateTime, default=datetime.utcnow, onupdate=datetime.utcnow)

    # Relationship with SOAPNote
    soap_note = db.relationship('SOAPNote', backref=db.backref('chunk_summaries', lazy=True))

    def __repr__(self):
        return f'<ChunkSummary {self.id} for SOAP Note {self.soap_note_id}>' 